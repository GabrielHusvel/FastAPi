{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 1: Crie uma aplicação simples em FastAPI que utilize o modelo GPT-2 da HuggingFace para gerar textos a partir de uma entrada fornecida via requisição HTTP.\n",
    "\n",
    "O aplicativo deve:\n",
    "\n",
    "Receber uma frase de entrada como JSON.\n",
    "Utilizar a biblioteca transformers do HuggingFace para gerar um texto de saída.\n",
    "Retornar o texto gerado em uma resposta HTTP.\n",
    "O que é esperado:\n",
    "\n",
    "O aplicativo deve gerar uma continuação de texto a partir de uma frase de entrada e retornar a resposta formatada como JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Carregar o pipeline GPT-2\n",
    "text_generator = pipeline(\"text-generation\", model=\"gpt2\", device=0)\n",
    "set_seed(42)  \n",
    "\n",
    "class InputText(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/generate-text/\")\n",
    "def generate_text(input_text: InputText):\n",
    "    if not input_text.text:\n",
    "        raise HTTPException(status_code=400, detail=\"Input text is required.\")\n",
    "    # Gerar texto\n",
    "    generated = text_generator(input_text.text, max_length=50, num_return_sequences=1)\n",
    "    return {\"input\": input_text.text, \"output\": generated[0][\"generated_text\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 2: Crie um aplicativo FastAPI que utiliza o modelo de tradução Helsinki-NLP/opus-mt-en-fr da HuggingFace para traduzir textos do inglês para o francês.\n",
    "\n",
    "A aplicação deve:\n",
    "\n",
    "Receber um texto em inglês via uma requisição HTTP.\n",
    "Traduzir o texto para o francês utilizando o modelo de tradução.\n",
    "Retornar o texto traduzido em uma resposta JSON.\n",
    "O que é esperado:\n",
    "\n",
    "A API deve receber um texto em inglês e retornar sua tradução para o francês, processando tanto frases curtas quanto textos mais longos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Carregar o pipeline de tradução\n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\", device=0)\n",
    "\n",
    "class TranslationInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/translate/\")\n",
    "def translate_text(input_text: TranslationInput):\n",
    "    if not input_text.text:\n",
    "        raise HTTPException(status_code=400, detail=\"Input text is required.\")\n",
    "    # Traduzir texto\n",
    "    translated = translator(input_text.text, max_length=400)\n",
    "    return {\"input\": input_text.text, \"translation\": translated[0][\"translation_text\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 3: Com base na API desenvolvida na Questão 2 (Parte1), explique as principais limitações do modelo de tradução utilizado.\n",
    "\n",
    "Enumere e discuta:\n",
    "\n",
    "Limitações quanto à precisão da tradução.\n",
    "Desafios de tempo de resposta e desempenho em grande escala.\n",
    "Restrições de custo e escalabilidade.\n",
    "Limitações na tradução de gírias, expressões idiomáticas ou linguagem de contexto.\n",
    "\n",
    "R.\n",
    "\n",
    "Precisão da Tradução:\n",
    "Não obtive a validação mais correta, visto que tive de traduzir do francês para o inglês novamente, mas os textos estavam precisos.\n",
    "\n",
    "Desenpenho:\n",
    "Aumento do tempo de resposta com textos longos se aplicaria caso quisessemos traduzir um livro, testei com vários paragrafos e obtive resposta instantânea.\n",
    "\n",
    "Custo e Escalabilidade:\n",
    "Hardware para operações em larga escala seria um \"problema\" caso estivessemos em alguma cloud, custos associados a servidores com GPUs.\n",
    "\n",
    "Contexto:\n",
    "Ao inserir gírias e abreviações como (Chill out, goat, dope, on fleek, you\"u\", because\"bc\", easy\"ez\") o modelo traduziu corretamente, diferente do que acontece em tradutores normais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 4: Com base no modelo GPT-2 utilizado na Questão 1 (Parte 1), explique as principais limitações do modelo no contexto da geração de texto.\n",
    "\n",
    "Discuta:\n",
    "\n",
    "A coerência do texto gerado.\n",
    "Possíveis falhas ou incoerências geradas por LLMs.\n",
    "Desempenho e questões de latência.\n",
    "Limitações na geração de conteúdo apropriado.\n",
    "\n",
    "R. \n",
    "\n",
    "Coerência do Texto:\n",
    "O texto certamente vai divergir ou ser incoerente no que foi solicitado, mesmo com um prompt simples o modelo vai escrever com base no seu embeding que não é tão bem calibrado.\n",
    "\n",
    "Falhas em Geração:\n",
    "percebi alguamas falhas na pontuação como uma única contra barra sem muito sentido, ou uma única aspas duplas, tabém sem contexto.\n",
    "\n",
    "Desempenho:\n",
    "Viés garantido. final de geração de texto aleatório.\n",
    "Boa rapidez na resposta.☻\n",
    "\n",
    "Geração de Conteúdo Apropriado:\n",
    "\n",
    "Falta de controle sobre o tom, por ser um modelo com um abismo de discrepância em relação aos atuais, é certeza de não se o conteúdo totalmente correto, \n",
    "\n",
    "Exemplo de resposta que o modelo me forneceu com um conteúdo não tão apropriado.\n",
    "{\n",
    "    \"input\": \"Say the names of countrys in Brazil?\",     \n",
    "    \"output\": \"Say the names of countrys in Brazil?\\n\\nThe United States is the only country where a man wants his wife or sister to have a baby and wants them to be free, a person in Brazil told me.\\n\\nIn the case\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 1: Desenvolva um protótipo utilizando LangChain que simule um chatbot simples com Fake LLM.\n",
    "\n",
    "A aplicação deve:\n",
    "\n",
    "Receber um input de texto via FastAPI.\n",
    "Retornar uma resposta simulada pelo Fake LLM.\n",
    "O que é esperado:\n",
    "\n",
    "O protótipo deve simular um chatbot básico que responde a perguntas pré-definidas. \n",
    "A arquitetura deve ser simples, e você deve explicar a importância de usar Fake LLM para testes rápidos. \n",
    "Desenhe um diagrama simples da arquitetura do aplicativo, detalhando as principais etapas do fluxo de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from langchain.llms import FakeListLLM\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Respostas pré-definidas\n",
    "responses = [\n",
    "    \"Olá! Como posso ajudá-lo hoje?\",\n",
    "    \"Estou bem, obrigado por perguntar!\",\n",
    "    \"Tenha um ótimo dia!\"\n",
    "]\n",
    "\n",
    "# Configurar o FakeLLM\n",
    "fake_llm = FakeListLLM(responses=responses)\n",
    "\n",
    "class InputText(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/chatbot/\")\n",
    "async def chatbot(input: InputText):\n",
    "    response = fake_llm(input.text)\n",
    "    return {\"response\": response}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+------------+          +----------+         +-----------+\n",
    "| User Input | ----->   | FastAPI  | ----->  | Fake LLM  |\n",
    "+------------+          +----------+         +-----------+\n",
    "      ^                                                    |\n",
    "      +----------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 2: Desenvolva um aplicativo que utilize LangChain para integrar a API da OpenAI.\n",
    "\n",
    "O aplicativo deve:\n",
    "\n",
    "Receber um texto em inglês via FastAPI.\n",
    "Traduzir o texto para o francês utilizando um modelo da OpenAI via LangChain.\n",
    "Retornar o texto traduzido em uma resposta JSON.\n",
    "O que é esperado:\n",
    "\n",
    "O aplicativo deve funcionar como uma API de tradução, semelhante à questão 2 (Parte 1), mas utilizando a OpenAI via LangChain. \n",
    "A aplicação deve gerenciar as chamadas à API da OpenAI e retornar a tradução com baixa latência.\n",
    "Forneça um diagrama da arquitetura da aplicação, destacando os componentes principais, como FastAPI, LangChain, e OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar variáveis de ambiente\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Modelo de entrada\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# Configuração do OpenAI LangChain\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "@app.post(\"/translate/\")\n",
    "async def translate_to_french(input: TextInput):\n",
    "    \"\"\"\n",
    "    Traduz texto do inglês para o francês usando OpenAI via LangChain\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Criar o prompt e obter a tradução\n",
    "        message = HumanMessage(content=f\"Translate this to French: {input.text}\")\n",
    "        response = llm.invoke([message])\n",
    "        return {\"original_text\": input.text, \"translated_text\": response.content}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+------------+          +----------+          +-----------+         +----------+\n",
    "| User Input | ----->   | FastAPI  | ----->   | LangChain | ----->  | OpenAI   |\n",
    "+------------+          +----------+          +-----------+         +----------+\n",
    "      ^                                                                            |\n",
    "      +----------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 3: Crie uma API semelhante à Questão 2 (Parte 2), mas que utilize o modelo Helsinki-NLP/opus-mt-en-de da HuggingFace para traduzir textos do inglês para o alemão.\n",
    "\n",
    "A aplicação deve:\n",
    "\n",
    "Receber um texto em inglês via FastAPI.\n",
    "Utilizar o LangChain para gerenciar as chamadas ao modelo HuggingFace.\n",
    "Retornar o texto traduzido para o alemão como resposta JSON.\n",
    "O que é esperado:\n",
    "\n",
    "O objetivo é que a aplicação funcione de maneira semelhante às Questões 2 (Parte 1) e 2 (Parte 2), mas desta vez integrando LangChain com HuggingFace. \n",
    "O modelo a ser utilizado deve ser o Helsinki-NLP/opus-mt-en-de.\n",
    "Forneça um diagrama detalhado da arquitetura da aplicação, destacando as interações entre FastAPI, LangChain, e HuggingFace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from transformers import pipeline\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Carregar o modelo de tradução\n",
    "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\", device=0)\n",
    "\n",
    "# Modelo de entrada\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/translate/\")\n",
    "async def translate_to_german(input: TextInput):\n",
    "    \"\"\"\n",
    "    Traduz texto do inglês para o alemão usando Helsinki-NLP diretamente com transformers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obter a tradução\n",
    "        result = translator(input.text, max_length=400)\n",
    "        translated_text = result[0][\"translation_text\"]\n",
    "        return {\"original_text\": input.text, \"translated_text\": translated_text}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+------------+          +----------+          +-----------+         +---------------+\n",
    "| User Input | ----->   | FastAPI  | ----->   | LangChain | ----->  | HuggingFace   |\n",
    "+------------+          +----------+          +-----------+         +---------------+\n",
    "      ^                                                                                 |\n",
    "      +---------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 4: Com base na implementação da Questão 2 (Parte 2), explique as principais limitações de utilizar LangChain para integrar a API da OpenAI.\n",
    "\n",
    "Discuta os seguintes aspectos:\n",
    "\n",
    "Latência de resposta.\n",
    "Limites de uso da API da OpenAI.\n",
    "Desafios de escalabilidade e custo.\n",
    "Qualidade das traduções geradas em comparação com outros modelos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\gabri\\projects\\tps\\tp2_datadriven\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gabri\\projects\\tps\\tp2_datadriven\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gabri\\projects\\tps\\tp2_datadriven\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Aspecto                                          Descrição\n",
      "0     Latência de resposta  É importante lembrar que terá o processamento ...\n",
      "1    Limites de uso da API  O limite de uso é conforme o modelo selecionad...\n",
      "2   Escalabilidade e custo  A melhor estratégia é usar o modelo necessário...\n",
      "3  Qualidade das traduções  os modelos da openai tem uma qualidade impecáv...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/quest4.csv', sep=';')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Questões 4 e 5: Limitações\n",
    "LangChain + OpenAI:\n",
    "\n",
    "Latência de Resposta: As chamadas à API da OpenAI podem ser lentas dependendo da carga do servidor.\n",
    "Limites de Uso: A OpenAI impõe limites de requisição e custos por token, dificultando a escalabilidade.\n",
    "Custo: O uso frequente da API pode ser caro para aplicações em larga escala.\n",
    "Qualidade: Traduções podem ser inconsistentes comparadas a modelos especializados.\n",
    "LangChain + HuggingFace:\n",
    "\n",
    "Desempenho: Processamento local pode ser mais lento sem hardware apropriado (GPU).\n",
    "Recursos Computacionais: Modelos grandes exigem mais memória e poder computacional.\n",
    "Flexibilidade: Ajustar o modelo requer conhecimento técnico avançado.\n",
    "Comparação com API: A API HuggingFace pode ser mais simples, mas é paga para uso em larga escala.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 5: Com base na aplicação desenvolvida na 3 (Parte 2), explique as limitações de usar LangChain para integrar o modelo HuggingFace de tradução. \n",
    "\n",
    "Discuta aspectos como:\n",
    "\n",
    "Desempenho e tempo de resposta.\n",
    "Consumo de recursos computacionais.\n",
    "Possíveis limitações no ajuste fino do modelo.\n",
    "Comparação com o uso direto da API HuggingFace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 6: Com base nas questões 1-2 (Parte 1) e 2-3 (Parte 2), desenvolva uma tabela comparativa que aborde os seguintes critérios:\n",
    "\n",
    "Facilidade de uso/configuração.\n",
    "Latência e desempenho.\n",
    "Flexibilidade para diferentes modelos.\n",
    "Custo e escalabilidade.\n",
    "Adequação para protótipos versus aplicações em produção.\n",
    "A comparação deve ser apresentada em formato de tabela, com colunas dedicadas a cada critério e linhas comparando FastAPI puro com LangChain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questão 6: Tabela Comparativa\n",
    "Critério\tFastAPI Puro\tLangChain\n",
    "Facilidade de Uso\tSimples e direto\tMais configuração, mas modular.\n",
    "Latência\tDepende da integração\tPode aumentar devido à camada adicional.\n",
    "Flexibilidade\tLimitada a bibliotecas\tSuporte para várias LLMs e integrações.\n",
    "Custo\tBaixo\tPode ser alto dependendo das integrações.\n",
    "Adequação\tBom para protótipos\tExcelente para produção e flexibilidade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
